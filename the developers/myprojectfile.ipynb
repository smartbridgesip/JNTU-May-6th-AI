{"nbformat_minor": 2, "cells": [{"source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 48}, {"source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_063c8c7ef9d84124a47dc8a6a3ad3478 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='LTRKr4XHWV_TMcfEpVKNjtQqmHvw1MgbM4X_R_zwd-5x',\n    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = client_063c8c7ef9d84124a47dc8a6a3ad3478.get_object(Bucket='deeplearningproject-donotdelete-pr-alexj8kyq2vbyt',Key='data (1).csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf = pd.read_csv(body)\ndf.head()\n\n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0    842302         M        17.99         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0     ...               17.33           184.60      2019.0            0.1622   \n1     ...               23.41           158.80      1956.0            0.1238   \n2     ...               25.53           152.50      1709.0            0.1444   \n3     ...               26.50            98.87       567.7            0.2098   \n4     ...               16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  Unnamed: 32  \n0                  0.11890          NaN  \n1                  0.08902          NaN  \n2                  0.08758          NaN  \n3                  0.17300          NaN  \n4                  0.07678          NaN  \n\n[5 rows x 33 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n      <th>Unnamed: 32</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 33 columns</p>\n</div>"}, "execution_count": 49, "metadata": {}}], "execution_count": 49}, {"source": "df.describe()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "                 id  radius_mean  texture_mean  perimeter_mean    area_mean  \\\ncount  5.690000e+02   569.000000    569.000000      569.000000   569.000000   \nmean   3.037183e+07    14.127292     19.289649       91.969033   654.889104   \nstd    1.250206e+08     3.524049      4.301036       24.298981   351.914129   \nmin    8.670000e+03     6.981000      9.710000       43.790000   143.500000   \n25%    8.692180e+05    11.700000     16.170000       75.170000   420.300000   \n50%    9.060240e+05    13.370000     18.840000       86.240000   551.100000   \n75%    8.813129e+06    15.780000     21.800000      104.100000   782.700000   \nmax    9.113205e+08    28.110000     39.280000      188.500000  2501.000000   \n\n       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\ncount       569.000000        569.000000      569.000000           569.000000   \nmean          0.096360          0.104341        0.088799             0.048919   \nstd           0.014064          0.052813        0.079720             0.038803   \nmin           0.052630          0.019380        0.000000             0.000000   \n25%           0.086370          0.064920        0.029560             0.020310   \n50%           0.095870          0.092630        0.061540             0.033500   \n75%           0.105300          0.130400        0.130700             0.074000   \nmax           0.163400          0.345400        0.426800             0.201200   \n\n       symmetry_mean     ...       texture_worst  perimeter_worst  \\\ncount     569.000000     ...          569.000000       569.000000   \nmean        0.181162     ...           25.677223       107.261213   \nstd         0.027414     ...            6.146258        33.602542   \nmin         0.106000     ...           12.020000        50.410000   \n25%         0.161900     ...           21.080000        84.110000   \n50%         0.179200     ...           25.410000        97.660000   \n75%         0.195700     ...           29.720000       125.400000   \nmax         0.304000     ...           49.540000       251.200000   \n\n        area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\ncount   569.000000        569.000000         569.000000       569.000000   \nmean    880.583128          0.132369           0.254265         0.272188   \nstd     569.356993          0.022832           0.157336         0.208624   \nmin     185.200000          0.071170           0.027290         0.000000   \n25%     515.300000          0.116600           0.147200         0.114500   \n50%     686.500000          0.131300           0.211900         0.226700   \n75%    1084.000000          0.146000           0.339100         0.382900   \nmax    4254.000000          0.222600           1.058000         1.252000   \n\n       concave points_worst  symmetry_worst  fractal_dimension_worst  \\\ncount            569.000000      569.000000               569.000000   \nmean               0.114606        0.290076                 0.083946   \nstd                0.065732        0.061867                 0.018061   \nmin                0.000000        0.156500                 0.055040   \n25%                0.064930        0.250400                 0.071460   \n50%                0.099930        0.282200                 0.080040   \n75%                0.161400        0.317900                 0.092080   \nmax                0.291000        0.663800                 0.207500   \n\n       Unnamed: 32  \ncount          0.0  \nmean           NaN  \nstd            NaN  \nmin            NaN  \n25%            NaN  \n50%            NaN  \n75%            NaN  \nmax            NaN  \n\n[8 rows x 32 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>symmetry_mean</th>\n      <th>...</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n      <th>Unnamed: 32</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>5.690000e+02</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>...</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>569.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.037183e+07</td>\n      <td>14.127292</td>\n      <td>19.289649</td>\n      <td>91.969033</td>\n      <td>654.889104</td>\n      <td>0.096360</td>\n      <td>0.104341</td>\n      <td>0.088799</td>\n      <td>0.048919</td>\n      <td>0.181162</td>\n      <td>...</td>\n      <td>25.677223</td>\n      <td>107.261213</td>\n      <td>880.583128</td>\n      <td>0.132369</td>\n      <td>0.254265</td>\n      <td>0.272188</td>\n      <td>0.114606</td>\n      <td>0.290076</td>\n      <td>0.083946</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.250206e+08</td>\n      <td>3.524049</td>\n      <td>4.301036</td>\n      <td>24.298981</td>\n      <td>351.914129</td>\n      <td>0.014064</td>\n      <td>0.052813</td>\n      <td>0.079720</td>\n      <td>0.038803</td>\n      <td>0.027414</td>\n      <td>...</td>\n      <td>6.146258</td>\n      <td>33.602542</td>\n      <td>569.356993</td>\n      <td>0.022832</td>\n      <td>0.157336</td>\n      <td>0.208624</td>\n      <td>0.065732</td>\n      <td>0.061867</td>\n      <td>0.018061</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>8.670000e+03</td>\n      <td>6.981000</td>\n      <td>9.710000</td>\n      <td>43.790000</td>\n      <td>143.500000</td>\n      <td>0.052630</td>\n      <td>0.019380</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.106000</td>\n      <td>...</td>\n      <td>12.020000</td>\n      <td>50.410000</td>\n      <td>185.200000</td>\n      <td>0.071170</td>\n      <td>0.027290</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.156500</td>\n      <td>0.055040</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.692180e+05</td>\n      <td>11.700000</td>\n      <td>16.170000</td>\n      <td>75.170000</td>\n      <td>420.300000</td>\n      <td>0.086370</td>\n      <td>0.064920</td>\n      <td>0.029560</td>\n      <td>0.020310</td>\n      <td>0.161900</td>\n      <td>...</td>\n      <td>21.080000</td>\n      <td>84.110000</td>\n      <td>515.300000</td>\n      <td>0.116600</td>\n      <td>0.147200</td>\n      <td>0.114500</td>\n      <td>0.064930</td>\n      <td>0.250400</td>\n      <td>0.071460</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>9.060240e+05</td>\n      <td>13.370000</td>\n      <td>18.840000</td>\n      <td>86.240000</td>\n      <td>551.100000</td>\n      <td>0.095870</td>\n      <td>0.092630</td>\n      <td>0.061540</td>\n      <td>0.033500</td>\n      <td>0.179200</td>\n      <td>...</td>\n      <td>25.410000</td>\n      <td>97.660000</td>\n      <td>686.500000</td>\n      <td>0.131300</td>\n      <td>0.211900</td>\n      <td>0.226700</td>\n      <td>0.099930</td>\n      <td>0.282200</td>\n      <td>0.080040</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8.813129e+06</td>\n      <td>15.780000</td>\n      <td>21.800000</td>\n      <td>104.100000</td>\n      <td>782.700000</td>\n      <td>0.105300</td>\n      <td>0.130400</td>\n      <td>0.130700</td>\n      <td>0.074000</td>\n      <td>0.195700</td>\n      <td>...</td>\n      <td>29.720000</td>\n      <td>125.400000</td>\n      <td>1084.000000</td>\n      <td>0.146000</td>\n      <td>0.339100</td>\n      <td>0.382900</td>\n      <td>0.161400</td>\n      <td>0.317900</td>\n      <td>0.092080</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>9.113205e+08</td>\n      <td>28.110000</td>\n      <td>39.280000</td>\n      <td>188.500000</td>\n      <td>2501.000000</td>\n      <td>0.163400</td>\n      <td>0.345400</td>\n      <td>0.426800</td>\n      <td>0.201200</td>\n      <td>0.304000</td>\n      <td>...</td>\n      <td>49.540000</td>\n      <td>251.200000</td>\n      <td>4254.000000</td>\n      <td>0.222600</td>\n      <td>1.058000</td>\n      <td>1.252000</td>\n      <td>0.291000</td>\n      <td>0.663800</td>\n      <td>0.207500</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows \u00d7 32 columns</p>\n</div>"}, "execution_count": 50, "metadata": {}}], "execution_count": 50}, {"source": "df.isnull().any()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "id                         False\ndiagnosis                  False\nradius_mean                False\ntexture_mean               False\nperimeter_mean             False\narea_mean                  False\nsmoothness_mean            False\ncompactness_mean           False\nconcavity_mean             False\nconcave points_mean        False\nsymmetry_mean              False\nfractal_dimension_mean     False\nradius_se                  False\ntexture_se                 False\nperimeter_se               False\narea_se                    False\nsmoothness_se              False\ncompactness_se             False\nconcavity_se               False\nconcave points_se          False\nsymmetry_se                False\nfractal_dimension_se       False\nradius_worst               False\ntexture_worst              False\nperimeter_worst            False\narea_worst                 False\nsmoothness_worst           False\ncompactness_worst          False\nconcavity_worst            False\nconcave points_worst       False\nsymmetry_worst             False\nfractal_dimension_worst    False\nUnnamed: 32                 True\ndtype: bool"}, "execution_count": 51, "metadata": {}}], "execution_count": 51}, {"source": "y=df.iloc[:,1].values", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 52}, {"source": "x_train[2]", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([-0.16778028,  0.71464588, -0.19387105, -0.25539924, -0.63244883,\n       -0.50278686, -0.49189629, -0.36906285,  0.01374189, -0.79419772,\n       -0.59959192,  0.25866693, -0.54084065, -0.42328115, -0.87980701,\n       -0.65225395, -0.33334744,  0.03169449, -0.49377709, -0.67098121,\n       -0.22138004,  1.02260198, -0.23370102, -0.31134325, -0.70483428,\n       -0.53047387, -0.18135392,  0.23344935, -0.04271939, -0.85592426])"}, "execution_count": 96, "metadata": {}}], "execution_count": 96}, {"source": "y.reshape(-1,1)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['B'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['M'],\n       ['B']], dtype=object)"}, "execution_count": 54, "metadata": {}}], "execution_count": 54}, {"source": "x=df.iloc[:,2:32].values\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 55}, {"source": "x", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[  1.79900000e+01,   1.03800000e+01,   1.22800000e+02, ...,\n          2.65400000e-01,   4.60100000e-01,   1.18900000e-01],\n       [  2.05700000e+01,   1.77700000e+01,   1.32900000e+02, ...,\n          1.86000000e-01,   2.75000000e-01,   8.90200000e-02],\n       [  1.96900000e+01,   2.12500000e+01,   1.30000000e+02, ...,\n          2.43000000e-01,   3.61300000e-01,   8.75800000e-02],\n       ..., \n       [  1.66000000e+01,   2.80800000e+01,   1.08300000e+02, ...,\n          1.41800000e-01,   2.21800000e-01,   7.82000000e-02],\n       [  2.06000000e+01,   2.93300000e+01,   1.40100000e+02, ...,\n          2.65000000e-01,   4.08700000e-01,   1.24000000e-01],\n       [  7.76000000e+00,   2.45400000e+01,   4.79200000e+01, ...,\n          0.00000000e+00,   2.87100000e-01,   7.03900000e-02]])"}, "execution_count": 56, "metadata": {}}], "execution_count": 56}, {"source": "x.shape", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(569, 30)"}, "execution_count": 57, "metadata": {}}], "execution_count": 57}, {"source": "from sklearn.preprocessing import LabelEncoder \n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 58}, {"source": "lb1=LabelEncoder()\ny=lb1.fit_transform(y)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 59}, {"source": "y", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n       0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n       1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n       1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0])"}, "execution_count": 60, "metadata": {}}], "execution_count": 60}, {"source": "from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 61}, {"source": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 62}, {"source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 63}, {"source": "model=Sequential()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 64}, {"source": "model.add(Dense(10,input_dim=30))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 65}, {"source": "model.add(Dense(10))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 66}, {"source": "model.add(Dense(1,activation=\"sigmoid\"))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 67}, {"source": "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 68}, {"source": "model.fit(x_train,y_train,epochs=100,batch_size=32)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Epoch 1/100\n455/455 [==============================] - 0s 285us/step - loss: 0.9357 - acc: 0.2813\nEpoch 2/100\n455/455 [==============================] - 0s 45us/step - loss: 0.6674 - acc: 0.5670\nEpoch 3/100\n455/455 [==============================] - 0s 45us/step - loss: 0.4988 - acc: 0.7868\nEpoch 4/100\n455/455 [==============================] - 0s 78us/step - loss: 0.3902 - acc: 0.8637\nEpoch 5/100\n455/455 [==============================] - 0s 45us/step - loss: 0.3100 - acc: 0.8967\nEpoch 6/100\n455/455 [==============================] - 0s 45us/step - loss: 0.2530 - acc: 0.9209\nEpoch 7/100\n455/455 [==============================] - 0s 108us/step - loss: 0.2097 - acc: 0.9363\nEpoch 8/100\n455/455 [==============================] - 0s 45us/step - loss: 0.1771 - acc: 0.9407\nEpoch 9/100\n455/455 [==============================] - 0s 43us/step - loss: 0.1517 - acc: 0.9495\nEpoch 10/100\n455/455 [==============================] - 0s 43us/step - loss: 0.1340 - acc: 0.9626\nEpoch 11/100\n455/455 [==============================] - 0s 102us/step - loss: 0.1212 - acc: 0.9648\nEpoch 12/100\n455/455 [==============================] - 0s 51us/step - loss: 0.1112 - acc: 0.9692\nEpoch 13/100\n455/455 [==============================] - 0s 44us/step - loss: 0.1037 - acc: 0.9758\nEpoch 14/100\n455/455 [==============================] - 0s 105us/step - loss: 0.0976 - acc: 0.9780\nEpoch 15/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0922 - acc: 0.9802\nEpoch 16/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0881 - acc: 0.9802\nEpoch 17/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0851 - acc: 0.9846\nEpoch 18/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0820 - acc: 0.9846\nEpoch 19/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0792 - acc: 0.9868\nEpoch 20/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0774 - acc: 0.9868\nEpoch 21/100\n455/455 [==============================] - 0s 107us/step - loss: 0.0754 - acc: 0.9824\nEpoch 22/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0733 - acc: 0.9846\nEpoch 23/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0718 - acc: 0.9824\nEpoch 24/100\n455/455 [==============================] - 0s 108us/step - loss: 0.0703 - acc: 0.9824\nEpoch 25/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0689 - acc: 0.9824\nEpoch 26/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0678 - acc: 0.9846\nEpoch 27/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0669 - acc: 0.9890\nEpoch 28/100\n455/455 [==============================] - 0s 105us/step - loss: 0.0657 - acc: 0.9912\nEpoch 29/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0647 - acc: 0.9912\nEpoch 30/100\n455/455 [==============================] - 0s 46us/step - loss: 0.0638 - acc: 0.9912\nEpoch 31/100\n455/455 [==============================] - 0s 108us/step - loss: 0.0631 - acc: 0.9912\nEpoch 32/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0624 - acc: 0.9890\nEpoch 33/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0616 - acc: 0.9890\nEpoch 34/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0610 - acc: 0.9890\nEpoch 35/100\n455/455 [==============================] - 0s 106us/step - loss: 0.0603 - acc: 0.9890\nEpoch 36/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0597 - acc: 0.9890\nEpoch 37/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0594 - acc: 0.9890\nEpoch 38/100\n455/455 [==============================] - 0s 106us/step - loss: 0.0588 - acc: 0.9890\nEpoch 39/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0583 - acc: 0.9890\nEpoch 40/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0579 - acc: 0.9890\nEpoch 41/100\n455/455 [==============================] - 0s 108us/step - loss: 0.0575 - acc: 0.9890\nEpoch 42/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0569 - acc: 0.9890\nEpoch 43/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0568 - acc: 0.9890\nEpoch 44/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0562 - acc: 0.9890\nEpoch 45/100\n455/455 [==============================] - 0s 110us/step - loss: 0.0557 - acc: 0.9890\nEpoch 46/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0553 - acc: 0.9890\nEpoch 47/100\n455/455 [==============================] - 0s 42us/step - loss: 0.0548 - acc: 0.9890\nEpoch 48/100\n455/455 [==============================] - 0s 103us/step - loss: 0.0546 - acc: 0.9890\nEpoch 49/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0543 - acc: 0.9890\nEpoch 50/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0543 - acc: 0.9890\nEpoch 51/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0537 - acc: 0.9890\nEpoch 52/100\n455/455 [==============================] - 0s 123us/step - loss: 0.0536 - acc: 0.9868\nEpoch 53/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0532 - acc: 0.9868\nEpoch 54/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0529 - acc: 0.9890\nEpoch 55/100\n455/455 [==============================] - 0s 104us/step - loss: 0.0526 - acc: 0.9890\nEpoch 56/100\n455/455 [==============================] - 0s 46us/step - loss: 0.0523 - acc: 0.9890\nEpoch 57/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0519 - acc: 0.9890\nEpoch 58/100\n455/455 [==============================] - 0s 109us/step - loss: 0.0516 - acc: 0.9890\nEpoch 59/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0515 - acc: 0.9890\nEpoch 60/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0513 - acc: 0.9890\nEpoch 61/100\n455/455 [==============================] - 0s 42us/step - loss: 0.0510 - acc: 0.9890\nEpoch 62/100\n455/455 [==============================] - 0s 115us/step - loss: 0.0509 - acc: 0.9890\nEpoch 63/100\n455/455 [==============================] - 0s 42us/step - loss: 0.0502 - acc: 0.9890\nEpoch 64/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0501 - acc: 0.9890\nEpoch 65/100\n455/455 [==============================] - 0s 101us/step - loss: 0.0503 - acc: 0.9912\nEpoch 66/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0499 - acc: 0.9912\nEpoch 67/100\n455/455 [==============================] - 0s 54us/step - loss: 0.0492 - acc: 0.9890\nEpoch 68/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0492 - acc: 0.9912\nEpoch 69/100\n455/455 [==============================] - 0s 97us/step - loss: 0.0491 - acc: 0.9912\nEpoch 70/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0485 - acc: 0.9912\nEpoch 71/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0483 - acc: 0.9912\nEpoch 72/100\n455/455 [==============================] - 0s 115us/step - loss: 0.0480 - acc: 0.9912\nEpoch 73/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0477 - acc: 0.9912\nEpoch 74/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0478 - acc: 0.9890\nEpoch 75/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0475 - acc: 0.9890\nEpoch 76/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0471 - acc: 0.9912\nEpoch 77/100\n455/455 [==============================] - 0s 46us/step - loss: 0.0470 - acc: 0.9912\nEpoch 78/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0465 - acc: 0.9890\nEpoch 79/100\n455/455 [==============================] - 0s 104us/step - loss: 0.0470 - acc: 0.9890\nEpoch 80/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0464 - acc: 0.9868\nEpoch 81/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0464 - acc: 0.9890\nEpoch 82/100\n455/455 [==============================] - 0s 102us/step - loss: 0.0463 - acc: 0.9868\nEpoch 83/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0456 - acc: 0.9890\nEpoch 84/100\n"}, {"output_type": "stream", "name": "stdout", "text": "455/455 [==============================] - 0s 44us/step - loss: 0.0464 - acc: 0.9912\nEpoch 85/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0456 - acc: 0.9912\nEpoch 86/100\n455/455 [==============================] - 0s 106us/step - loss: 0.0452 - acc: 0.9912\nEpoch 87/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0454 - acc: 0.9912\nEpoch 88/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0448 - acc: 0.9912\nEpoch 89/100\n455/455 [==============================] - 0s 113us/step - loss: 0.0442 - acc: 0.9912\nEpoch 90/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0442 - acc: 0.9912\nEpoch 91/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0437 - acc: 0.9912\nEpoch 92/100\n455/455 [==============================] - 0s 43us/step - loss: 0.0436 - acc: 0.9912\nEpoch 93/100\n455/455 [==============================] - 0s 103us/step - loss: 0.0443 - acc: 0.9912\nEpoch 94/100\n455/455 [==============================] - 0s 44us/step - loss: 0.0445 - acc: 0.9912\nEpoch 95/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0453 - acc: 0.9824\nEpoch 96/100\n455/455 [==============================] - 0s 100us/step - loss: 0.0450 - acc: 0.9868\nEpoch 97/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0429 - acc: 0.9890\nEpoch 98/100\n455/455 [==============================] - 0s 52us/step - loss: 0.0431 - acc: 0.9868\nEpoch 99/100\n455/455 [==============================] - 0s 45us/step - loss: 0.0427 - acc: 0.9890\nEpoch 100/100\n455/455 [==============================] - 0s 97us/step - loss: 0.0425 - acc: 0.9912\n"}, {"output_type": "execute_result", "data": {"text/plain": "<keras.callbacks.History at 0x7f6a1c47f278>"}, "execution_count": 69, "metadata": {}}], "execution_count": 69}, {"source": "y_pred=model.predict(x_test)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 70}, {"source": "score=model.evaluate(x_train,y_train)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "455/455 [==============================] - 0s 61us/step\n"}], "execution_count": 71}, {"source": "print('accuracy :'+str(score[1]*100))", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "accuracy :99.1208791209\n"}], "execution_count": 72}, {"source": "\n\n\ny_pred=model.predict(x_test)\ny_pred=(y_pred>0.5)\ny_pred", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[False],\n       [ True],\n       [ True],\n       [False],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [ True],\n       [ True],\n       [False],\n       [ True],\n       [ True],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [ True],\n       [ True],\n       [ True],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [ True],\n       [ True],\n       [ True],\n       [ True],\n       [ True],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True]], dtype=bool)"}, "execution_count": 73, "metadata": {}}], "execution_count": 73}, {"source": "model.save(\"project.h5\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 74}, {"source": "!tar -zcvf project.tgz project.h5", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "project.h5\r\n"}], "execution_count": 75}, {"source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 76}, {"source": "kgf={\n    \"accesskey\":\"LmCcqPrfv6iJNBm8JpMml-FR7UzMng_5sHMUkPgjxGv8\",\n    \"instance_id\": \"723b47aa-1cfc-4096-9aa4-f4164edabfef\",\n  \"password\": \"14b7bdbf-ddc4-4c80-823f-bf64be74118b\",\n  \"url\": \"https://eu-gb.ml.cloud.ibm.com\",\n  \"username\": \"fc498606-2244-4781-a53e-845935a312d9\"\n}   \n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 77}, {"source": "client=WatsonMachineLearningAPIClient(kgf)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 78}, {"source": "metadata={\n    client.repository.ModelMetaNames.NAME:\"keras model\",\n    client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES:[{'name':'keras','version':'2.1.3'}],\n     client.repository.ModelMetaNames.FRAMEWORK_NAME:\"tensorflow\",\n     client.repository.ModelMetaNames.FRAMEWORK_VERSION:\"1.5\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 79}, {"source": "model_details=client.repository.store_model(model=\"project.tgz\",meta_props=metadata)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 80}, {"source": "client.repository.list_models()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "------------------------------------  -----------  ------------------------  --------------\nGUID                                  NAME         CREATED                   FRAMEWORK\n2f827446-d9f9-4fa5-9a2e-d7b3b317b701  keras model  2019-05-24T18:09:28.121Z  tensorflow-1.5\n23c4aa44-56d9-405e-afd8-7a2565d97e28  keras model  2019-05-24T18:07:17.729Z  tensorflow-1.5\n1255b5db-91e9-4df1-ad1b-7dd337c9092b  keras model  2019-05-24T17:50:04.832Z  tensorflow-1.5\n1985470b-81a5-4df1-acf5-290229ec8ff7  keras model  2019-05-24T07:26:47.461Z  tensorflow-1.5\n8e6dc922-50d8-48ae-95a2-58b34d6d661f  keras model  2019-05-22T07:54:42.795Z  tensorflow-1.5\n515b8f6e-2359-4ee5-a8df-6eb544394c4a  keras model  2019-05-17T10:03:41.685Z  tensorflow-1.5\n------------------------------------  -----------  ------------------------  --------------\n"}], "execution_count": 81}, {"source": "model_uid=model_details[\"metadata\"][\"guid\"]\nmodel_deploy=client.deployments.create(artifact_uid=model_uid,name='classification')", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '2f827446-d9f9-4fa5-9a2e-d7b3b317b701' started\n\n#######################################################################################\n\n\nINITIALIZING\nDEPLOY_IN_PROGRESS\nDEPLOY_SUCCESS\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='dcbb9cf5-9348-4cad-b158-51dc4200d43d'\n------------------------------------------------------------------------------------------------\n\n\n"}], "execution_count": 82}, {"source": "scoring_endpoint=client.deployments.get_scoring_url(model_deploy)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 83}, {"source": "scoring_endpoint", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'https://eu-gb.ml.cloud.ibm.com/v3/wml_instances/723b47aa-1cfc-4096-9aa4-f4164edabfef/deployments/dcbb9cf5-9348-4cad-b158-51dc4200d43d/online'"}, "execution_count": 84, "metadata": {}}], "execution_count": 84}, {"source": "from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "0.96491228070175439"}, "execution_count": 85, "metadata": {}}], "execution_count": 85}, {"source": "n=model.predict_classes((np.array([[-0.16778028,  0.71464588, -0.19387105, -0.25539924, -0.63244883,\n       -0.50278686, -0.49189629, -0.36906285,  0.01374189, -0.79419772,\n       -0.59959192,  0.25866693, -0.54084065, -0.42328115, -0.87980701,\n       -0.65225395, -0.33334744,  0.03169449, -0.49377709, -0.67098121,\n       -0.22138004,  1.02260198, -0.23370102, -0.31134325, -0.70483428,\n       -0.53047387, -0.18135392,  0.23344935, -0.04271939, -0.85592426]])))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 97}, {"source": "sc.transform(np.array([[9.504,10.38,122.8,1001,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189]]))", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[-1.30642051, -2.062581  ,  1.29602947,  1.00675312,  1.65446818,\n         3.40545988,  2.75178408,  2.61073574,  2.28401635,  2.3185689 ,\n         2.40497343, -0.55805077,  2.73229564,  2.37197354, -0.21688153,\n         1.4477612 ,  0.74914564,  0.68933052,  1.10331953,  0.97465928,\n         1.92027056, -1.35208145,  2.34910987,  2.04933964,  1.33466412,\n         2.84030385,  2.25559979,  2.33766767,  2.7733405 ,  2.18393991]])"}, "execution_count": 98, "metadata": {}}], "execution_count": 98}, {"source": "n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[0]], dtype=int32)"}, "execution_count": 99, "metadata": {}}], "execution_count": 99}, {"source": "client.deployments.", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "------------------------------------  --------------  ------  --------------  ------------------------  --------------  -------------\nGUID                                  NAME            TYPE    STATE           CREATED                   FRAMEWORK       ARTIFACT TYPE\n39a2bc19-ed10-4d5b-83d5-954ac90d004a  classification  online  DEPLOY_SUCCESS  2019-05-24T17:50:09.135Z  tensorflow-1.5  model\n12bf78e3-98b6-44b9-a6f9-1caa66f28e7c  classification  online  DEPLOY_SUCCESS  2019-05-24T07:26:50.291Z  tensorflow-1.5  model\nff487534-e5ab-40ad-90dd-c6788603b1f6  classification  online  DEPLOY_SUCCESS  2019-05-22T07:56:46.244Z  tensorflow-1.5  model\n6647bc44-767b-4bd5-9b96-8128a12dae0c  ann_reg         online  DEPLOY_SUCCESS  2019-05-17T10:13:24.552Z  tensorflow-1.5  model\n------------------------------------  --------------  ------  --------------  ------------------------  --------------  -------------\n"}], "execution_count": 6}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}